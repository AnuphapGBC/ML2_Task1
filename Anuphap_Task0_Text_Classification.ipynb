{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzQ3osPAbnQE"
      },
      "source": [
        "# Text Classification\n",
        "## This notebook outlines the usage of NLP Feature extraction (CountVectorizer, TfidfVectorizer) in classification of text documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd3f16arbnQG"
      },
      "source": [
        "### Import all the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kjD0Gn2p6-j",
        "outputId": "6434110e-2ff8-4f46-c00f-a084c62c7f0f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/bng-\n",
            "[nltk_data]     anuphap_c/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "from time import time\n",
        "import logging\n",
        "import nltk\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from gensim.models import Word2Vec, Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvFmi-QU4uWz"
      },
      "source": [
        "### Choose a few categories fro the entire 20 categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XkC4M0nl0_1o"
      },
      "outputs": [],
      "source": [
        "# Load some categories from the training set\n",
        "categories = [\n",
        "    'alt.atheism',\n",
        "    'talk.religion.misc',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4qmxAXO0_zK",
        "outputId": "f17697d8-1016-4741-9ce9-402faf279d40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 20 newsgroups dataset for categories:\n",
            "['alt.atheism', 'talk.religion.misc']\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading 20 newsgroups dataset for categories:\")\n",
        "print(categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbrHjHJZ42wh"
      },
      "source": [
        "### Fetch documents for these 2 categories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJA5lNGR0_wu",
        "outputId": "ea3d8ba6-0933-425e-fd4f-7375ae2d544e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "857 documents\n",
            "2 categories\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = fetch_20newsgroups(subset='train', categories=categories)\n",
        "print(f\"{len(data.filenames)} documents\")\n",
        "print(f\"{len(data.target_names)} categories\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMHGzIBH5ITh"
      },
      "source": [
        " ### Make Word2Vec and Doc2Vec work (Credit: Stackoverflow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g2npi1LSo09g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return [np.maximum(self.model.wv[word], 0) if word in self.model.wv else np.zeros(self.model.vector_size) for word in X]\n",
        "\n",
        "class Doc2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return [np.maximum(self.model.infer_vector(word_tokenize(doc)), 0) if len(word_tokenize(doc)) > 0 else np.zeros(self.model.vector_size) for doc in X]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zsU1fNvQyI6A"
      },
      "outputs": [],
      "source": [
        "word2vec_model = Word2Vec(sentences=[word_tokenize(text) for text in data.data])\n",
        "\n",
        "tagged_data = [TaggedDocument(words=word_tokenize(doc), tags=[str(i)]) for i, doc in enumerate(data.data)]\n",
        "doc2vec_model = Doc2Vec()\n",
        "doc2vec_model.build_vocab(tagged_data)\n",
        "doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbVwAIhR492T"
      },
      "source": [
        "### Algo and Extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0M9NZ8bxdVQd"
      },
      "outputs": [],
      "source": [
        "algorithms = [\n",
        "    ('Multinomial Na√Øve Bayes', MultinomialNB()),\n",
        "    ('Logistic Regression', LogisticRegression()),\n",
        "    ('Support Vector Machines', SVC()),\n",
        "    ('Decision Trees', DecisionTreeClassifier()),\n",
        "]\n",
        "\n",
        "extractors = [\n",
        "    ('Bag-of-Words', CountVectorizer()),\n",
        "    ('N-grams', CountVectorizer(ngram_range=(1, 2))),\n",
        "    ('word2vec', Word2VecTransformer(model=word2vec_model)),\n",
        "    ('doc2vec', Doc2VecTransformer(model=doc2vec_model))\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YYgoKOz5bMr"
      },
      "source": [
        "### Combine pipeline, Set parameters = all (avoid error), do gridsearch, and benchmarking (performance, and accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axCCITDKdVOe",
        "outputId": "1950fe25-1cc6-4262-c811-0adbf63a2bf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "for algo_name, algo in algorithms:\n",
        "    for extractor_name, extractor in extractors:\n",
        "        pipeline = Pipeline([\n",
        "            ('vect', extractor),\n",
        "            ('clf', algo),\n",
        "        ])\n",
        "\n",
        "        parameters = {}\n",
        "\n",
        "        grid_search = GridSearchCV(pipeline, parameters, cv=5,\n",
        "                                   n_jobs=-1, verbose=1)\n",
        "\n",
        "        start_time = time()\n",
        "        grid_search.fit(data.data, data.target)\n",
        "        end_time = time()\n",
        "\n",
        "        execution_time = end_time - start_time\n",
        "        best_score = grid_search.best_score_\n",
        "        best_parameters = grid_search.best_estimator_.get_params()\n",
        "\n",
        "        results.append({\n",
        "            'Algorithm': algo_name,\n",
        "            'Feature Extractor': extractor_name,\n",
        "            'Best Score': best_score,\n",
        "            'Execution Time': execution_time,\n",
        "            'Best Parameters': best_parameters\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgYQQDvu6wTy"
      },
      "source": [
        "### Save in tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RZwmAJweG9g",
        "outputId": "38cf7324-f433-4e6f-cd1b-b81095f7d9ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best:\n",
            "{'Algorithm': 'Logistic Regression',\n",
            " 'Best Parameters': {'clf': LogisticRegression(),\n",
            "                     'clf__C': 1.0,\n",
            "                     'clf__class_weight': None,\n",
            "                     'clf__dual': False,\n",
            "                     'clf__fit_intercept': True,\n",
            "                     'clf__intercept_scaling': 1,\n",
            "                     'clf__l1_ratio': None,\n",
            "                     'clf__max_iter': 100,\n",
            "                     'clf__multi_class': 'auto',\n",
            "                     'clf__n_jobs': None,\n",
            "                     'clf__penalty': 'l2',\n",
            "                     'clf__random_state': None,\n",
            "                     'clf__solver': 'lbfgs',\n",
            "                     'clf__tol': 0.0001,\n",
            "                     'clf__verbose': 0,\n",
            "                     'clf__warm_start': False,\n",
            "                     'memory': None,\n",
            "                     'steps': [('vect', CountVectorizer(ngram_range=(1, 2))),\n",
            "                               ('clf', LogisticRegression())],\n",
            "                     'vect': CountVectorizer(ngram_range=(1, 2)),\n",
            "                     'vect__analyzer': 'word',\n",
            "                     'vect__binary': False,\n",
            "                     'vect__decode_error': 'strict',\n",
            "                     'vect__dtype': <class 'numpy.int64'>,\n",
            "                     'vect__encoding': 'utf-8',\n",
            "                     'vect__input': 'content',\n",
            "                     'vect__lowercase': True,\n",
            "                     'vect__max_df': 1.0,\n",
            "                     'vect__max_features': None,\n",
            "                     'vect__min_df': 1,\n",
            "                     'vect__ngram_range': (1, 2),\n",
            "                     'vect__preprocessor': None,\n",
            "                     'vect__stop_words': None,\n",
            "                     'vect__strip_accents': None,\n",
            "                     'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                     'vect__tokenizer': None,\n",
            "                     'vect__vocabulary': None,\n",
            "                     'verbose': False},\n",
            " 'Best Score': 0.9451652386780905,\n",
            " 'Execution Time': 4.198161840438843,\n",
            " 'Feature Extractor': 'N-grams'}\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "best_configuration = max(results, key=lambda x: x['Best Score'])\n",
        "\n",
        "print(\"Best:\")\n",
        "pprint(best_configuration)\n",
        "\n",
        "with open('results.txt', 'w') as file:\n",
        "\n",
        "    file.write(\"Best:\\n\")\n",
        "    file.write(tabulate([best_configuration], headers='keys', tablefmt='grid'))\n",
        "\n",
        "    file.write(\"\\n\\nAll Possibles:\\n\")\n",
        "    file.write(tabulate(results, headers='keys', tablefmt='grid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HjDRNezxTRQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
